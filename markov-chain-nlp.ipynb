{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016466,
     "end_time": "2020-11-25T06:32:34.619602",
     "exception": false,
     "start_time": "2020-11-25T06:32:34.603136",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&amp;id=1zSJwAUxWv5bxyYLmYPNi-s6M_Wq5iWXh\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013027,
     "end_time": "2020-11-25T06:32:34.646276",
     "exception": false,
     "start_time": "2020-11-25T06:32:34.633249",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Importing tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-11-25T06:32:34.679819Z",
     "iopub.status.busy": "2020-11-25T06:32:34.679055Z",
     "iopub.status.idle": "2020-11-25T06:32:36.522676Z",
     "shell.execute_reply": "2020-11-25T06:32:36.521941Z"
    },
    "papermill": {
     "duration": 1.863124,
     "end_time": "2020-11-25T06:32:36.522810",
     "exception": false,
     "start_time": "2020-11-25T06:32:34.659686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\sumed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013469,
     "end_time": "2020-11-25T06:32:36.550629",
     "exception": false,
     "start_time": "2020-11-25T06:32:36.537160",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Reading every Sherlock Holmes adventure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-25T06:32:36.592712Z",
     "iopub.status.busy": "2020-11-25T06:32:36.591797Z",
     "iopub.status.idle": "2020-11-25T06:32:37.036576Z",
     "shell.execute_reply": "2020-11-25T06:32:37.035811Z"
    },
    "papermill": {
     "duration": 0.47249,
     "end_time": "2020-11-25T06:32:37.036707",
     "exception": false,
     "start_time": "2020-11-25T06:32:36.564217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines =  215021\n"
     ]
    }
   ],
   "source": [
    "story_path = \"sherlock\\sherlock\"\n",
    "\n",
    "\n",
    "def read_all_stories(story_path):\n",
    "    txt = []\n",
    "    for _, _, files in os.walk(story_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(story_path, file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line=='----------': break\n",
    "                    if line!='':txt.append(line)\n",
    "    return txt\n",
    "        \n",
    "stories = read_all_stories(story_path)\n",
    "print(\"number of lines = \", len(stories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013801,
     "end_time": "2020-11-25T06:32:37.065221",
     "exception": false,
     "start_time": "2020-11-25T06:32:37.051420",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-25T06:32:37.103246Z",
     "iopub.status.busy": "2020-11-25T06:32:37.102503Z",
     "iopub.status.idle": "2020-11-25T06:33:20.288100Z",
     "shell.execute_reply": "2020-11-25T06:33:20.288915Z"
    },
    "papermill": {
     "duration": 43.209558,
     "end_time": "2020-11-25T06:33:20.289125",
     "exception": false,
     "start_time": "2020-11-25T06:32:37.079567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words =  2332247\n"
     ]
    }
   ],
   "source": [
    "def clean_txt(txt):\n",
    "    cleaned_txt = []\n",
    "    for line in txt:\n",
    "        line = line.lower()\n",
    "        line = re.sub(r\"[,.\\\"\\'!@#$%^&*(){}?/;`~:<>+=-\\\\]\", \"\", line)\n",
    "        tokens = word_tokenize(line)\n",
    "        words = [word for word in tokens if word.isalpha()]\n",
    "        cleaned_txt+=words\n",
    "    return cleaned_txt\n",
    "\n",
    "cleaned_stories = clean_txt(stories)\n",
    "print(\"number of words = \", len(cleaned_stories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014325,
     "end_time": "2020-11-25T06:33:20.318770",
     "exception": false,
     "start_time": "2020-11-25T06:33:20.304445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating the Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-25T06:33:20.362368Z",
     "iopub.status.busy": "2020-11-25T06:33:20.361452Z",
     "iopub.status.idle": "2020-11-25T06:33:20.364871Z",
     "shell.execute_reply": "2020-11-25T06:33:20.364135Z"
    },
    "papermill": {
     "duration": 0.031699,
     "end_time": "2020-11-25T06:33:20.364992",
     "exception": false,
     "start_time": "2020-11-25T06:33:20.333293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_markov_model(cleaned_stories, n_gram=2):\n",
    "    markov_model = {}\n",
    "    for i in range(len(cleaned_stories)-n_gram-1):\n",
    "        curr_state, next_state = \"\", \"\"\n",
    "        for j in range(n_gram):\n",
    "            curr_state += cleaned_stories[i+j] + \" \"\n",
    "            next_state += cleaned_stories[i+j+n_gram] + \" \"\n",
    "        curr_state = curr_state[:-1]\n",
    "        next_state = next_state[:-1]\n",
    "        if curr_state not in markov_model:\n",
    "            markov_model[curr_state] = {}\n",
    "            markov_model[curr_state][next_state] = 1\n",
    "        else:\n",
    "            if next_state in markov_model[curr_state]:\n",
    "                markov_model[curr_state][next_state] += 1\n",
    "            else:\n",
    "                markov_model[curr_state][next_state] = 1\n",
    "    \n",
    "    # calculating transition probabilities\n",
    "    for curr_state, transition in markov_model.items():\n",
    "        total = sum(transition.values())\n",
    "        for state, count in transition.items():\n",
    "            markov_model[curr_state][state] = count/total\n",
    "        \n",
    "    return markov_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-25T06:33:20.559411Z",
     "iopub.status.busy": "2020-11-25T06:33:20.533665Z",
     "iopub.status.idle": "2020-11-25T06:33:28.813035Z",
     "shell.execute_reply": "2020-11-25T06:33:28.812187Z"
    },
    "papermill": {
     "duration": 8.433005,
     "end_time": "2020-11-25T06:33:28.813164",
     "exception": false,
     "start_time": "2020-11-25T06:33:20.380159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "markov_model = make_markov_model(cleaned_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-25T06:33:28.850231Z",
     "iopub.status.busy": "2020-11-25T06:33:28.849185Z",
     "iopub.status.idle": "2020-11-25T06:33:28.853728Z",
     "shell.execute_reply": "2020-11-25T06:33:28.852853Z"
    },
    "papermill": {
     "duration": 0.025469,
     "end_time": "2020-11-25T06:33:28.853878",
     "exception": false,
     "start_time": "2020-11-25T06:33:28.828409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of states =  208716\n"
     ]
    }
   ],
   "source": [
    "print(\"number of states = \", len(markov_model.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-25T06:33:28.891766Z",
     "iopub.status.busy": "2020-11-25T06:33:28.890848Z",
     "iopub.status.idle": "2020-11-25T06:33:28.894326Z",
     "shell.execute_reply": "2020-11-25T06:33:28.894970Z"
    },
    "papermill": {
     "duration": 0.025345,
     "end_time": "2020-11-25T06:33:28.895122",
     "exception": false,
     "start_time": "2020-11-25T06:33:28.869777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All possible transitions from 'the game' state: \n",
      "\n",
      "{'your letter': 0.02702702702702703, 'was up': 0.09009009009009009, 'is afoot': 0.036036036036036036, 'for the': 0.036036036036036036, 'was in': 0.02702702702702703, 'is hardly': 0.02702702702702703, 'would have': 0.036036036036036036, 'is up': 0.06306306306306306, 'is and': 0.036036036036036036, 'in their': 0.036036036036036036, 'was whist': 0.036036036036036036, 'in that': 0.036036036036036036, 'the lack': 0.036036036036036036, 'for all': 0.06306306306306306, 'may wander': 0.02702702702702703, 'now a': 0.02702702702702703, 'my own': 0.02702702702702703, 'at any': 0.02702702702702703, 'mr holmes': 0.02702702702702703, 'ay whats': 0.02702702702702703, 'my friend': 0.02702702702702703, 'fairly by': 0.02702702702702703, 'is not': 0.02702702702702703, 'was not': 0.02702702702702703, 'was afoot': 0.036036036036036036, 'worth it': 0.02702702702702703, 'you are': 0.02702702702702703, 'i am': 0.02702702702702703, 'now count': 0.02702702702702703}\n"
     ]
    }
   ],
   "source": [
    "print(\"All possible transitions from 'the game' state: \\n\")\n",
    "print(markov_model['the game'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016112,
     "end_time": "2020-11-25T06:33:28.927707",
     "exception": false,
     "start_time": "2020-11-25T06:33:28.911595",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generating Sherlock Holmes stories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-25T06:33:28.970771Z",
     "iopub.status.busy": "2020-11-25T06:33:28.969903Z",
     "iopub.status.idle": "2020-11-25T06:33:28.973704Z",
     "shell.execute_reply": "2020-11-25T06:33:28.972947Z"
    },
    "papermill": {
     "duration": 0.02932,
     "end_time": "2020-11-25T06:33:28.973833",
     "exception": false,
     "start_time": "2020-11-25T06:33:28.944513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_story(markov_model, limit=100, start='my god'):\n",
    "    n = 0\n",
    "    curr_state = start\n",
    "    next_state = None\n",
    "    story = \"\"\n",
    "    story+=curr_state+\" \"\n",
    "    while n<limit:\n",
    "        next_state = random.choices(list(markov_model[curr_state].keys()),\n",
    "                                    list(markov_model[curr_state].values()))\n",
    "        \n",
    "        curr_state = next_state[0]\n",
    "        story+=curr_state+\" \"\n",
    "        n+=1\n",
    "    return story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-25T06:33:29.021500Z",
     "iopub.status.busy": "2020-11-25T06:33:29.020560Z",
     "iopub.status.idle": "2020-11-25T06:33:29.025821Z",
     "shell.execute_reply": "2020-11-25T06:33:29.024728Z"
    },
    "papermill": {
     "duration": 0.034888,
     "end_time": "2020-11-25T06:33:29.025999",
     "exception": false,
     "start_time": "2020-11-25T06:33:28.991111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.  dear holmes i thought at first he had then had a companion so that his alluding to it had a wire that there have been times when the trap pulled up on that and you must acknowledge that there can not be \n",
      "1.  dear holmes said i desire to see the matter through miller hill this morning i had to ascertain whether you be alive or dead there is no either upon his forehead holmes sat up with a ladys card upon her face more \n",
      "2.  dear holmes what do you say that the nature and a hard enough fight to get it home with you mr mcmurdo may i ask you lord holdhurst lord holdhurst i shall write to warn the county authorities that something we can \n",
      "3.  dear holmes if i knew where the rooms that he would not let the matter rest till i have had the least doubt in my mind to put this matter it is impossible to distinguish one from the bag and compared the \n",
      "4.  dear holmes i have said all right cabby said he to actually go off with the knocker holmes rose motioning us to buy the geese off you no names please said holmes to the cottage hurried the inmates were warned of your \n",
      "5.  dear holmes i ejaculated oh there could be seen but what had happened and of what you very much i shall be happy to have had work enough already through you however mr melas i should wish to have a quick eye \n",
      "6.  dear holmes i ejaculated well really it seems to me it must be so damn fine if i must say in court you will not improve any reputation which i seemed to bristle up with the automatic slots are drawn why should \n",
      "7.  dear holmes i have a respect for you are very rich in orchids on the moor my nerves but my mind is the most it is not selfishness or conceit said he i have heard him prowling about the house with me \n",
      "8.  dear holmes i thought you had reason to remember his saying that a man in uniform advanced into the room that you have read the accusation the least unlikely exactly now mr gilchrist we are all practical jokes and that i dare \n",
      "9.  dear holmes it is as you seem to have fallen into the hall and the heath was opened quite so why did you not deduce something from that time she began to stare at me again the gentle voice had a sharp \n",
      "10.  dear holmes and tell us the quick eye for faces holmes it may be so in such matches and little billy james and the date are on the back and an uncertain foot then glancing quickly round he was standing by so \n",
      "11.  dear holmes said i think your grace that matters can hardly be matched among all the world holmes i cried you say that you and she were now nearly five weeks have elapsed then and nothing of value there there was no \n",
      "12.  dear holmes i ejaculated as a leader he was reading his legend yes i should like just to the right shoulder do what you and your attendance will be required until then we can dismiss the case or not i it was \n",
      "13.  dear holmes you are going to force my way in which they had hired having first arranged that the base of the room was necessary for we are driven back to it i think not more than thirty but his arm stopped \n",
      "14.  dear holmes i have said ostentatiously affectionate but she was never as i think this is intolerable they must have left it the window had been practising the jump he returned carrying his jumping shoes which counterfeited the tracks of the same \n",
      "15.  dear holmes i ejaculated as a hansom could bring him a certain hour it seemed that we were a family mannerism can be traced in these matters and he transmitted a remark upon short sight and he followed me as governess i \n",
      "16.  dear holmes what do you make of that fellow answered my visitor and i will introduce you to be more explicit i will conceal nothing from them for the annoyance which i had supplied and hardly ever to discover the least clue \n",
      "17.  dear holmes and tell us all go round the world i had a vague sound which resembled the distant baying of a whaler which was due to apoplexy you see anything there has been a long scrutiny out of the fire it \n",
      "18.  dear holmes what do you think he would acquire no knowledge which is essential and what clues can i have lost my hold on the essentials stanley hopkins thats the story of the copier of the appearance of the other laughed i \n",
      "19.  dear holmes if i can get at first the steps by which he disentangled the most of it i realized how completely the evidence against himself for who is he here because i feel it in such a rascal though i have \n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(str(i)+\". \", generate_story(markov_model, start=\"dear holmes\", limit=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-25T06:33:29.073741Z",
     "iopub.status.busy": "2020-11-25T06:33:29.072261Z",
     "iopub.status.idle": "2020-11-25T06:33:29.078030Z",
     "shell.execute_reply": "2020-11-25T06:33:29.076963Z"
    },
    "papermill": {
     "duration": 0.033785,
     "end_time": "2020-11-25T06:33:29.078212",
     "exception": false,
     "start_time": "2020-11-25T06:33:29.044427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.  my dear fellow i congrat to milvertons housemaid good heavens cried my dear fellow said he there are \n",
      "1.  my dear watson and i think we have already lost so much may have been taken out and \n",
      "2.  my dear mycroft the problem which i had to wait for that though he carried a heavy stick \n",
      "3.  my dear sir there must have been dead a thousand sheep paths until we came to drive it \n",
      "4.  my dear fellow said he in answer to my friends the remark had a chat with him on \n",
      "5.  my dear sir cried the king of all i must leave you to your collection watson said he \n",
      "6.  my dear mr sherlock holmes i said if you can think a year at the most sympathetic and \n",
      "7.  my dear watson that i shall start off myself and then by no means the conception one forms \n",
      "8.  my dear fellow you know my boss barney gives me the impression of a few of my friends \n",
      "9.  my dear watson i take him on i would not be very much deeper and more human than \n",
      "10.  my dear watson theres my report no where can she be then it has been considered artistic i \n",
      "11.  my dear von bork i think that there was something on his violin sank into reveries devoured sandwiches \n",
      "12.  my dear girl it was not holmess nature to take me into waterloo at so i dressed and \n",
      "13.  my dear sir such a thing that i thought little enough of it get out of the ordinary \n",
      "14.  my dear fellow what can i do holmes glanced at his watch our rate at present it is \n",
      "15.  my dear fellow said our guide had left my chair and my client led me away at once \n",
      "16.  my dear mr mac the first words which were upon the scene of the bank directors with the \n",
      "17.  my dear sir you never heard of it was a little fat round fellow with a single pull \n",
      "18.  my dear watson we have done you very well in fact he answered laughing it was strange to \n",
      "19.  my dear watson should realize said barker quickly as he stood in the dock for a sucker from \n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(str(i)+\". \", generate_story(markov_model, start=\"my dear\", limit=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-25T06:33:29.127823Z",
     "iopub.status.busy": "2020-11-25T06:33:29.126733Z",
     "iopub.status.idle": "2020-11-25T06:33:29.131503Z",
     "shell.execute_reply": "2020-11-25T06:33:29.130427Z"
    },
    "papermill": {
     "duration": 0.03434,
     "end_time": "2020-11-25T06:33:29.131674",
     "exception": false,
     "start_time": "2020-11-25T06:33:29.097334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.  i would lay a long cork its appearance and the presence of our friend armstrong would have to \n",
      "1.  i would have you managed it will you i thought he might throw some light upon the fate \n",
      "2.  i would say nothing mind you think then that he had only two yesterday and one from southampton \n",
      "3.  i would engage a front room and sat down again and asked to give him some fresh cause \n",
      "4.  i would have you turned the handle but only to wash his hands and all the readers of \n",
      "5.  i would retire to my he always apologized to me to bear the unconscious man out he seems \n",
      "6.  i would spend my life hiking round the buildings he failed to rouse any sign of four we \n",
      "7.  i would cut off by the mode of progression observed by holmes and submitted to the left of \n",
      "8.  i would also urge that the measurements must refer to some spot to which the rest of your \n",
      "9.  i would give my process of deduction when i hear from godfreys own lips meanwhile i pass on \n",
      "10.  i would not have known which was the former who forced the unfortunate young man was instantly arrested \n",
      "11.  i would sooner have a savage i exclaimed in unfeigned admiration it is so high that he could \n",
      "12.  i would do what i can not bring one tinge of colour sprang to my feet would carry \n",
      "13.  i would move said the secretary sir i have but a vague vision of a dark cloak who \n",
      "14.  i would not intrude my services oh indeed said my new acquaintance to baker street we are estranged \n",
      "15.  i would say nothing of the cobra let me briefly give this note to my neighbor and gladder \n",
      "16.  i would have a housekeeper now but she could not have returned the lost sight of the spot \n",
      "17.  i would suggest that you make of these and it would be useful to him in the power \n",
      "18.  i would do it when they understand it when it would occur to a stand thats audley court \n",
      "19.  i would go round the sun was shining very brightly and yet its our own necks that may \n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(str(i)+\". \", generate_story(markov_model, start=\"i would\", limit=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-25T06:33:29.179898Z",
     "iopub.status.busy": "2020-11-25T06:33:29.178984Z",
     "iopub.status.idle": "2020-11-25T06:33:29.183451Z",
     "shell.execute_reply": "2020-11-25T06:33:29.182682Z"
    },
    "papermill": {
     "duration": 0.030943,
     "end_time": "2020-11-25T06:33:29.183636",
     "exception": false,
     "start_time": "2020-11-25T06:33:29.152693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the case in that case i perceive three of the poor folk who were really in this matter which i have only just come into the general scheme which he will have to be seen nor heard the sharp clang of the most busy men in it where i am sorry he said and i really do sometimes think that it might be the same distance from the north of ireland and to the latter view since the day before be connected with our buttoned up for it i am bound to go he handed them a shilling there was no sign of my heart he was always a bit of a strange bird no no the yew alley i saw the city and i hope that you are far from well she had left at a single bust the reading one and the drag is about to put to the salesman framed in the door of the time while the marks are perfectly correct said he speaking from the point of my revolver and laid it reverently under a bonnet on this planet so say the to suggest how the fugitive got away with the resident patient nothing has happened i \n"
     ]
    }
   ],
   "source": [
    "print(generate_story(markov_model, start=\"the case\", limit=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "duration": 59.473932,
   "end_time": "2020-11-25T06:33:29.314157",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-25T06:32:29.840225",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
